{"docstore/metadata": {"7807d40a-9bd7-47d4-93c8-be6abc19cc64": {"doc_hash": "afd9c09fcdc49bdccf616708e397b383b78871144cefe5ec9b27c13a8118764b"}, "e5e0bae8-2e1c-4557-bcec-1e41c326c16a": {"doc_hash": "848a5478bd2b08fb9e05e71b16295c0357cdb0d1856f122d445dfc8a43d3db74"}, "c35ec669-67f8-4f16-ada9-36ca0a3e6fb1": {"doc_hash": "e4e6377980ba88af60bdc52040b625c4f6a354eec99709ca15f8c0006bfc9bb9"}, "273b8b27-25a4-4f84-9d16-9e81ae5671ba": {"doc_hash": "1aad7585379cfbdc81ab611f39289281e593de9466a07148d81df7b0c3384a48"}, "d0fa23fc-fe85-4f43-ab4b-831bf7379211": {"doc_hash": "58ae2c3bd45818ca42daced23faab772b3c97e361e2391c35e69145925b146d9"}, "65bcb9f3-62c6-4522-a013-9c323311445b": {"doc_hash": "8f144ef790313dcd405891912758526255667adf6d88ea35f6703f8dbe9abcec"}, "db976d3a-d214-4fad-a0ed-df65f526d2eb": {"doc_hash": "a3c862399b5794315615ada93ffbb7f6f4bb3b8fe6c6805c5aa0c078bf89233c"}, "d0e84345-1999-4abf-8e9b-0738d11baf89": {"doc_hash": "05add458417c5019f90f8c4b5139c41b00f3cbaf589cdd7b703332b5abbfc59d"}, "c957d006-bd35-4e3d-b5c3-d3aa0cdad8f1": {"doc_hash": "1c0e52bbbb48421e4aadc208a2c8252e27243b1c0deab6efe570c3b7c34dbe26"}, "af7c7f88-49cd-4fdf-9f9c-60faf64d146b": {"doc_hash": "848a5478bd2b08fb9e05e71b16295c0357cdb0d1856f122d445dfc8a43d3db74", "ref_doc_id": "e5e0bae8-2e1c-4557-bcec-1e41c326c16a"}, "c34ffa4d-28d0-4b14-8d3c-66b810ff4368": {"doc_hash": "e4e6377980ba88af60bdc52040b625c4f6a354eec99709ca15f8c0006bfc9bb9", "ref_doc_id": "c35ec669-67f8-4f16-ada9-36ca0a3e6fb1"}, "da62b901-b718-4371-8e54-6f76832db60c": {"doc_hash": "1aad7585379cfbdc81ab611f39289281e593de9466a07148d81df7b0c3384a48", "ref_doc_id": "273b8b27-25a4-4f84-9d16-9e81ae5671ba"}, "f8974519-9de8-43e5-8f39-64297ec52724": {"doc_hash": "58ae2c3bd45818ca42daced23faab772b3c97e361e2391c35e69145925b146d9", "ref_doc_id": "d0fa23fc-fe85-4f43-ab4b-831bf7379211"}, "6ea81835-170a-4f9d-bcf7-bc37d3391f29": {"doc_hash": "8f144ef790313dcd405891912758526255667adf6d88ea35f6703f8dbe9abcec", "ref_doc_id": "65bcb9f3-62c6-4522-a013-9c323311445b"}, "4245f28a-63c0-40f6-a0e0-ddee17a0a172": {"doc_hash": "a3c862399b5794315615ada93ffbb7f6f4bb3b8fe6c6805c5aa0c078bf89233c", "ref_doc_id": "db976d3a-d214-4fad-a0ed-df65f526d2eb"}, "3f329327-21b3-449d-9d49-4cf115850918": {"doc_hash": "05add458417c5019f90f8c4b5139c41b00f3cbaf589cdd7b703332b5abbfc59d", "ref_doc_id": "d0e84345-1999-4abf-8e9b-0738d11baf89"}, "788fba01-a240-48a8-aee1-bb895e35fd3f": {"doc_hash": "1c0e52bbbb48421e4aadc208a2c8252e27243b1c0deab6efe570c3b7c34dbe26", "ref_doc_id": "c957d006-bd35-4e3d-b5c3-d3aa0cdad8f1"}}, "docstore/data": {"af7c7f88-49cd-4fdf-9f9c-60faf64d146b": {"__data__": {"id_": "af7c7f88-49cd-4fdf-9f9c-60faf64d146b", "embedding": null, "metadata": {"page_label": "2", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5e0bae8-2e1c-4557-bcec-1e41c326c16a", "node_type": null, "metadata": {"page_label": "2", "file_name": "Chapter1.pdf"}, "hash": "848a5478bd2b08fb9e05e71b16295c0357cdb0d1856f122d445dfc8a43d3db74"}}, "hash": "848a5478bd2b08fb9e05e71b16295c0357cdb0d1856f122d445dfc8a43d3db74", "text": "\u201cAll models are wrong, but some are useful.\u201d\n\u2014George Box\nThe book is distributed on the \u201cread \ufb01rst, buy later\u201d principle.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft", "start_char_idx": 0, "end_char_idx": 182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c34ffa4d-28d0-4b14-8d3c-66b810ff4368": {"__data__": {"id_": "c34ffa4d-28d0-4b14-8d3c-66b810ff4368", "embedding": null, "metadata": {"page_label": "3", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c35ec669-67f8-4f16-ada9-36ca0a3e6fb1", "node_type": null, "metadata": {"page_label": "3", "file_name": "Chapter1.pdf"}, "hash": "e4e6377980ba88af60bdc52040b625c4f6a354eec99709ca15f8c0006bfc9bb9"}}, "hash": "e4e6377980ba88af60bdc52040b625c4f6a354eec99709ca15f8c0006bfc9bb9", "text": "1 Introduction\n1.1 What is Machine Learning\nMachine learning is a sub\ufb01eld of computer science that is concerned with building algorithms\nwhich, to be useful, rely on a collection of examples of some phenomenon. These examples\ncan come from nature, be handcrafted by humans or generated by another algorithm.\nMachine learning can also be de\ufb01ned as the process of solving a practical problem by 1)\ngathering a dataset, and 2) algorithmically building a statistical model based on that dataset.\nThat statistical model is assumed to be used somehow to solve the practical problem.\nTo save keystrokes, I use the terms \u201clearning\u201d and \u201cmachine learning\u201d interchangeably.\n1.2 Types of Learning\nLearning can be supervised, semi-supervised, unsupervised and reinforcement.\n1.2.1 Supervised Learning\nInsupervised learning1, thedataset is the collection of labeled examples {(xi, yi)}N\ni=1.\nEach element xiamong Nis called a feature vector . A feature vector is a vector in which\neach dimension j= 1, . . . , Dcontains a value that describes the example somehow. That\nvalue is called a feature and is denoted as x(j). For instance, if each example xin our\ncollection represents a person, then the \ufb01rst feature, x(1), could contain height in cm, the\nsecond feature, x(2), could contain weight in kg, x(3)could contain gender, and so on. For all\nexamples in the dataset, the feature at position jin the feature vector always contains the\nsame kind of information. It means that if x(2)\nicontains weight in kg in some example xi,\nthenx(2)\nkwill also contain weight in kg in every example xk,k= 1, . . . , N. The label yican\nbe either an element belonging to a \ufb01nite set of classes{1,2, . . . , C}, or a real number, or a\nmore complex structure, like a vector, a matrix, a tree, or a graph. Unless otherwise stated,\nin this book yiis either one of a \ufb01nite set of classes or a real number2. You can see a class as\na category to which an example belongs. For instance, if your examples are email messages\nand your problem is spam detection, then you have two classes {spam, not _spam}.\nThe goal of a supervised learning algorithm is to use the dataset to produce a model\nthat takes a feature vector xas input and outputs information that allows deducing the label\nfor this feature vector. For instance, the model created using the dataset of people could\ntake as input a feature vector describing a person and output a probability that the person\nhas cancer.\n1If a term is in bold, that means that the term can be found in the index at the end of the book.\n2A real number is a quantity that can represent a distance along a line. Examples: 0,\u2212256 .34,1000,\n1000 .2.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 3", "start_char_idx": 0, "end_char_idx": 2710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "da62b901-b718-4371-8e54-6f76832db60c": {"__data__": {"id_": "da62b901-b718-4371-8e54-6f76832db60c", "embedding": null, "metadata": {"page_label": "4", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "273b8b27-25a4-4f84-9d16-9e81ae5671ba", "node_type": null, "metadata": {"page_label": "4", "file_name": "Chapter1.pdf"}, "hash": "1aad7585379cfbdc81ab611f39289281e593de9466a07148d81df7b0c3384a48"}}, "hash": "1aad7585379cfbdc81ab611f39289281e593de9466a07148d81df7b0c3384a48", "text": "1.2.2 Unsupervised Learning\nInunsupervised learning , the dataset is a collection of unlabeled examples {xi}N\ni=1.\nAgain, xis a feature vector, and the goal of an unsupervised learning algorithm is\nto create a modelthat takes a feature vector xas input and either transforms it into\nanother vector or into a value that can be used to solve a practical problem. For example,\ninclustering , the model returns the id of the cluster for each feature vector in the dataset.\nIndimensionality reduction , the output of the model is a feature vector that has fewer\nfeatures than the input x; inoutlier detection , the output is a real number that indicates\nhowxis di\ufb00erent from a \u201ctypical\u201d example in the dataset.\n1.2.3 Semi-Supervised Learning\nInsemi-supervised learning , the dataset contains both labeled and unlabeled examples.\nUsually, the quantity of unlabeled examples is much higher than the number of labeled\nexamples. The goal of a semi-supervised learning algorithm is the same as the goal of\nthe supervised learning algorithm. The hope here is that using many unlabeled examples can\nhelp the learning algorithm to \ufb01nd (we might say \u201cproduce\u201d or \u201ccompute\u201d) a better model.\nIt could look counter-intuitive that learning could bene\ufb01t from adding more unlabeled\nexamples. It seems like we add more uncertainty to the problem. However, when you add\nunlabeled examples, you add more information about your problem: a larger sample re\ufb02ects\nbetter the probability distribution the data we labeled came from. Theoretically, a learning\nalgorithm should be able to leverage this additional information.\n1.2.4 Reinforcement Learning\nReinforcement learning is a sub\ufb01eld of machine learning where the machine \u201clives\u201d in\nan environment and is capable of perceiving the stateof that environment as a vector of\nfeatures. The machine can execute actionsin every state. Di\ufb00erent actions bring di\ufb00erent\nrewardsand could also move the machine to another state of the environment. The goal of a\nreinforcement learning algorithm is to learn a policy.\nA policy is a function (similar to the model in supervised learning)\nthat takes the feature vector of a state as input and outputs an\noptimal action to execute in that state. The action is optimal if\nit maximizes the expected average reward .\nReinforcement learning solves a particular kind of problem where\ndecision making is sequential, and the goal is long-term, such as\ngame playing, robotics, resource management, or logistics. In this\nbook, I put emphasis on one-shot decision making where input\nexamples are independent of one another and the predictions made\nin the past. I leave reinforcement learning out of the scope of this book.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 4", "start_char_idx": 0, "end_char_idx": 2736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f8974519-9de8-43e5-8f39-64297ec52724": {"__data__": {"id_": "f8974519-9de8-43e5-8f39-64297ec52724", "embedding": null, "metadata": {"page_label": "5", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0fa23fc-fe85-4f43-ab4b-831bf7379211", "node_type": null, "metadata": {"page_label": "5", "file_name": "Chapter1.pdf"}, "hash": "58ae2c3bd45818ca42daced23faab772b3c97e361e2391c35e69145925b146d9"}}, "hash": "58ae2c3bd45818ca42daced23faab772b3c97e361e2391c35e69145925b146d9", "text": "1.3 How Supervised Learning Works\nIn this section, I brie\ufb02y explain how supervised learning works so that you have the picture\nof the whole process before we go into detail. I decided to use supervised learning as an\nexample because it\u2019s the type of machine learning most frequently used in practice.\nThe supervised learning process starts with gathering the data. The data for supervised\nlearning is a collection of pairs (input, output). Input could be anything, for example,\nemail messages, pictures, or sensor measurements. Outputs are usually real numbers, or\nlabels (e.g. \u201cspam\u201d, \u201cnot_spam\u201d, \u201ccat\u201d, \u201cdog\u201d, \u201cmouse\u201d, etc). In some cases, outputs are\nvectors (e.g., four coordinates of the rectangle around a person on the picture), sequences\n(e.g. [\u201cadjective\u201d, \u201cadjective\u201d, \u201cnoun\u201d] for the input \u201cbig beautiful car\u201d), or have some other\nstructure.\nLet\u2019s say the problem that you want to solve using supervised learning is spam detection.\nYou gather the data, for example, 10,000 email messages, each with a label either \u201cspam\u201d or\n\u201cnot_spam\u201d (you could add those labels manually or pay someone to do that for you). Now,\nyou have to convert each email message into a feature vector.\nThe data analyst decides, based on their experience, how to convert a real-world entity, such\nas an email message, into a feature vector. One common way to convert a text into a feature\nvector, called bag of words , is to take a dictionary of English words (let\u2019s say it contains\n20,000 alphabetically sorted words) and stipulate that in our feature vector:\n\u2022the \ufb01rst feature is equal to 1if the email message contains the word \u201ca\u201d; otherwise,\nthis feature is 0;\n\u2022thesecondfeatureisequalto 1iftheemailmessagecontainstheword\u201caaron\u201d; otherwise,\nthis feature equals 0;\n\u2022...\n\u2022the feature at position 20,000 is equal to 1if the email message contains the word\n\u201czulu\u201d; otherwise, this feature is equal to 0.\nYou repeat the above procedure for every email message in our collection, which gives\nus 10,000 feature vectors (each vector having the dimensionality of 20,000) and a label\n(\u201cspam\u201d/\u201cnot_spam\u201d).\nNow you have machine-readable input data, but the output labels are still in the form of\nhuman-readable text. Some learning algorithms require transforming labels into numbers.\nFor example, some algorithms require numbers like 0(to represent the label \u201cnot_spam\u201d)\nand1(to represent the label \u201cspam\u201d). The algorithm I use to illustrate supervised learning is\ncalled Support Vector Machine (SVM). This algorithm requires that the positive label (in\nour case it\u2019s \u201cspam\u201d) has the numeric value of +1(one), and the negative label (\u201cnot_spam\u201d)\nhas the value of\u22121(minus one).\nAt this point, you have a dataset and a learning algorithm , so you are ready to apply\nthe learning algorithm to the dataset to get the model.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 5", "start_char_idx": 0, "end_char_idx": 2858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6ea81835-170a-4f9d-bcf7-bc37d3391f29": {"__data__": {"id_": "6ea81835-170a-4f9d-bcf7-bc37d3391f29", "embedding": null, "metadata": {"page_label": "6", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65bcb9f3-62c6-4522-a013-9c323311445b", "node_type": null, "metadata": {"page_label": "6", "file_name": "Chapter1.pdf"}, "hash": "8f144ef790313dcd405891912758526255667adf6d88ea35f6703f8dbe9abcec"}}, "hash": "8f144ef790313dcd405891912758526255667adf6d88ea35f6703f8dbe9abcec", "text": "SVM sees every feature vector as a point in a high-dimensional space (in our case, space\nis 20,000-dimensional). The algorithm puts all feature vectors on an imaginary 20,000-\ndimensionalplotanddrawsanimaginary19,999-dimensionalline(a hyperplane )thatseparates\nexamples with positive labels from examples with negative labels. In machine learning, the\nboundary separating the examples of di\ufb00erent classes is called the decision boundary .\nThe equation of the hyperplane is given by two parameters , a real-valued vector wof the\nsame dimensionality as our input feature vector x, and a real number blike this:\nwx\u2212b= 0,\nwhere the expression wxmeans w(1)x(1)+w(2)x(2)+. . .+w(D)x(D), and Dis the number\nof dimensions of the feature vector x.\n(If some equations aren\u2019t clear to you right now, in Chapter 2 we revisit the math and\nstatistical concepts necessary to understand them. For the moment, try to get an intuition of\nwhat\u2019s happening here. It all becomes more clear after you read the next chapter.)\nNow, the predicted label for some input feature vector xis given like this:\ny= sign( wx\u2212b),\nwhere signis a mathematical operator that takes any value as input and returns +1if the\ninput is a positive number or \u22121if the input is a negative number.\nThe goal of the learning algorithm \u2014 SVM in this case \u2014 is to leverage the dataset and \ufb01nd\nthe optimal values w\u2217andb\u2217for parameters wandb. Once the learning algorithm identi\ufb01es\nthese optimal values, the model f(x)is then de\ufb01ned as:\nf(x) = sign( w\u2217x\u2212b\u2217)\nTherefore, to predict whether an email message is spam or not spam using an SVM model,\nyou have to take the text of the message, convert it into a feature vector, then multiply this\nvector by w\u2217, subtract b\u2217and take the sign of the result. This will give us the prediction ( +1\nmeans \u201cspam\u201d,\u22121means \u201cnot_spam\u201d).\nNow, how does the machine \ufb01nd w\u2217andb\u2217? It solves an optimization problem. Machines\nare good at optimizing functions under constraints.\nSo what are the constraints we want to satisfy here? First of all, we want the model to predict\nthe labels of our 10,000 examples correctly. Remember that each example i= 1, . . . , 10000is\ngiven by a pair (xi, yi), where xiis the feature vector of example iandyiis its label that\ntakes values either \u22121or+1. So the constraints are naturally:\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 6", "start_char_idx": 0, "end_char_idx": 2355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4245f28a-63c0-40f6-a0e0-ddee17a0a172": {"__data__": {"id_": "4245f28a-63c0-40f6-a0e0-ddee17a0a172", "embedding": null, "metadata": {"page_label": "7", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db976d3a-d214-4fad-a0ed-df65f526d2eb", "node_type": null, "metadata": {"page_label": "7", "file_name": "Chapter1.pdf"}, "hash": "a3c862399b5794315615ada93ffbb7f6f4bb3b8fe6c6805c5aa0c078bf89233c"}}, "hash": "a3c862399b5794315615ada93ffbb7f6f4bb3b8fe6c6805c5aa0c078bf89233c", "text": "x( 2)\nx(1)wx\u00a0\u2014\u00a0b\u00a0=\u00a00wx\u00a0\u2014\u00a0b\u00a0=\u00a01\nwx\u00a0\u2014\u00a0b\u00a0=\u00a0\u20141\nb\u00a0|| w||\u00a02\u00a0|| w||\u00a0Figure 1: An example of an SVM model for two-dimensional feature vectors.\nwxi\u2212b\u2265+1ifyi= +1 ,\nwxi\u2212b\u2264\u22121ifyi=\u22121.\nWe would also prefer that the hyperplane separates positive examples from negative ones with\nthe largest margin. The margin is the distance between the closest examples of two classes,\nas de\ufb01ned by the decision boundary. A large margin contributes to a better generalization ,\nthat is how well the model will classify new examples in the future. To achieve that, we need\nto minimize the Euclidean norm of wdenoted by\u2225w\u2225and given by\u2211\u2211D\nj=1(w(j))2.\nSo, the optimization problem that we want the machine to solve looks like this:\nMinimize\u2225w\u2225subject to yi(wxi\u2212b)\u22651fori= 1, . . . , N. The expression yi(wxi\u2212b)\u22651\nis just a compact way to write the above two constraints.\nThe solution of this optimization problem, given by w\u2217andb\u2217, is called the statistical\nmodel, or, simply, the model. The process of building the model is called training .\nFor two-dimensional feature vectors, the problem and the solution can be visualized as\nshown in Figure 1. The blue and orange circles represent, respectively, positive and negative\nexamples, and the line given by wx\u2212b= 0is the decision boundary.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 7", "start_char_idx": 0, "end_char_idx": 1316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f329327-21b3-449d-9d49-4cf115850918": {"__data__": {"id_": "3f329327-21b3-449d-9d49-4cf115850918", "embedding": null, "metadata": {"page_label": "8", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0e84345-1999-4abf-8e9b-0738d11baf89", "node_type": null, "metadata": {"page_label": "8", "file_name": "Chapter1.pdf"}, "hash": "05add458417c5019f90f8c4b5139c41b00f3cbaf589cdd7b703332b5abbfc59d"}}, "hash": "05add458417c5019f90f8c4b5139c41b00f3cbaf589cdd7b703332b5abbfc59d", "text": "Why, by minimizing the norm of w, do we \ufb01nd the highest margin between the two classes?\nGeometrically, the equations wx\u2212b= 1andwx\u2212b=\u22121de\ufb01ne two parallel hyperplanes, as\nyou see in Figure 1. The distance between these hyperplanes is given by2\n\u2225w\u2225, so the smaller\nthe norm\u2225w\u2225, the larger the distance between these two hyperplanes.\nThat\u2019s how Support Vector Machines work. This particular version of the algorithm builds\nthe so-called linear model . It\u2019s called linear because the decision boundary is a straight line\n(or a plane, or a hyperplane). SVM can also incorporate kernels that can make the decision\nboundary arbitrarily non-linear. In some cases, it could be impossible to perfectly separate\nthe two groups of points because of noise in the data, errors of labeling, or outliers (examples\nvery di\ufb00erent from a \u201ctypical\u201d example in the dataset). Another version of SVM can also\nincorporate a penalty hyperparameter3for misclassi\ufb01cation of training examples of speci\ufb01c\nclasses. We study the SVM algorithm in more detail in Chapter 3.\nAt this point, you should retain the following: any classi\ufb01cation learning algorithm that\nbuilds a model implicitly or explicitly creates a decision boundary. The decision boundary\ncan be straight, or curved, or it can have a complex form, or it can be a superposition of\nsome geometrical \ufb01gures. The form of the decision boundary determines the accuracy of\nthe model (that is the ratio of examples whose labels are predicted correctly). The form of\nthe decision boundary, the way it is algorithmically or mathematically computed based on\nthe training data, di\ufb00erentiates one learning algorithm from another.\nIn practice, there are two other essential di\ufb00erentiators of learning algorithms to consider:\nspeed of model building and prediction processing time. In many practical cases, you would\nprefer a learning algorithm that builds a less accurate model quickly. Additionally, you might\nprefer a less accurate model that is much quicker at making predictions.\n1.4 Why the Model Works on New Data\nWhy is a machine-learned model capable of predicting correctly the labels of new, previously\nunseen examples? To understand that, look at the plot in Figure 1. If two classes are\nseparable from one another by a decision boundary, then, obviously, examples that belong to\neach class are located in two di\ufb00erent subspaces which the decision boundary creates.\nIf the examples used for training were selected randomly, independently of one another, and\nfollowing the same procedure, then, statistically, it is more likely that the new negative\nexample will be located on the plot somewhere not too far from other negative examples.\nThe same concerns the new positive example: it will likelycome from the surroundings of\nother positive examples. In such a case, our decision boundary will still, with high probability ,\nseparate well new positive and negative examples from one another. For other, less likely\nsituations , our model will make errors, but because such situations are less likely, the number\nof errors will likely be smaller than the number of correct predictions.\n3A hyperparameter is a property of a learning algorithm, usually (but not always) having a numerical\nvalue. That value in\ufb02uences the way the algorithm works. Those values aren\u2019t learned by the algorithm itself\nfrom data. They have to be set by the data analyst before running the algorithm.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 8", "start_char_idx": 0, "end_char_idx": 3466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "788fba01-a240-48a8-aee1-bb895e35fd3f": {"__data__": {"id_": "788fba01-a240-48a8-aee1-bb895e35fd3f", "embedding": null, "metadata": {"page_label": "9", "file_name": "Chapter1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c957d006-bd35-4e3d-b5c3-d3aa0cdad8f1", "node_type": null, "metadata": {"page_label": "9", "file_name": "Chapter1.pdf"}, "hash": "1c0e52bbbb48421e4aadc208a2c8252e27243b1c0deab6efe570c3b7c34dbe26"}}, "hash": "1c0e52bbbb48421e4aadc208a2c8252e27243b1c0deab6efe570c3b7c34dbe26", "text": "Intuitively, the larger is the set of training examples, the more unlikely that the new examples\nwill be dissimilar to (and lie on the plot far from) the examples used for training.\nTo minimize the probability of making errors on new examples,\nthe SVM algorithm, by looking for the largest margin, explicitly\ntries to draw the decision boundary in such a way that it lies as\nfar as possible from examples of both classes.\nThe reader interested in knowing more about the learnability and\nunderstanding the close relationship between the model error, the\nsize of the training set, the form of the mathematical equation\nthat de\ufb01nes the model, and the time it takes to build the model\nis encouraged to read about the PAC learning . The PAC (for\n\u201cprobably approximately correct\u201d) learning theory helps to analyze whether and under what\nconditions a learning algorithm will probably output an approximately correct classi\ufb01er.\n1.5 Counterfeit Alert\nAs of April 2019, the Internet is full of counterfeit copies of my book, including printed\ncopies. To avoid buying counterfeit, I recommend following the links on the book\u2019s supporting\nwebsite themlbook.com. Even if you buy on Amazon, you should make sure that you buy\ndirectly from Amazon and not from a third-party seller.\nAndriy Burkov The Hundred-Page Machine Learning Book - Draft 9", "start_char_idx": 0, "end_char_idx": 1329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"e5e0bae8-2e1c-4557-bcec-1e41c326c16a": {"node_ids": ["af7c7f88-49cd-4fdf-9f9c-60faf64d146b"], "metadata": {"page_label": "2", "file_name": "Chapter1.pdf"}}, "c35ec669-67f8-4f16-ada9-36ca0a3e6fb1": {"node_ids": ["c34ffa4d-28d0-4b14-8d3c-66b810ff4368"], "metadata": {"page_label": "3", "file_name": "Chapter1.pdf"}}, "273b8b27-25a4-4f84-9d16-9e81ae5671ba": {"node_ids": ["da62b901-b718-4371-8e54-6f76832db60c"], "metadata": {"page_label": "4", "file_name": "Chapter1.pdf"}}, "d0fa23fc-fe85-4f43-ab4b-831bf7379211": {"node_ids": ["f8974519-9de8-43e5-8f39-64297ec52724"], "metadata": {"page_label": "5", "file_name": "Chapter1.pdf"}}, "65bcb9f3-62c6-4522-a013-9c323311445b": {"node_ids": ["6ea81835-170a-4f9d-bcf7-bc37d3391f29"], "metadata": {"page_label": "6", "file_name": "Chapter1.pdf"}}, "db976d3a-d214-4fad-a0ed-df65f526d2eb": {"node_ids": ["4245f28a-63c0-40f6-a0e0-ddee17a0a172"], "metadata": {"page_label": "7", "file_name": "Chapter1.pdf"}}, "d0e84345-1999-4abf-8e9b-0738d11baf89": {"node_ids": ["3f329327-21b3-449d-9d49-4cf115850918"], "metadata": {"page_label": "8", "file_name": "Chapter1.pdf"}}, "c957d006-bd35-4e3d-b5c3-d3aa0cdad8f1": {"node_ids": ["788fba01-a240-48a8-aee1-bb895e35fd3f"], "metadata": {"page_label": "9", "file_name": "Chapter1.pdf"}}}}