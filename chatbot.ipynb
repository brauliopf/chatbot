{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4496,"status":"ok","timestamp":1686511275562,"user":{"displayName":"Braulio Fernandes","userId":"15281912688788735537"},"user_tz":240},"id":"6I1eNDkvXYBD","outputId":"69dcd6e2-25eb-48b7-ea32-48bafd2ab79d"},"outputs":[],"source":["# !pip install llama-index\n","# !pip install openai\n","# !pip install pypdf\n","# !pip install torch\n","# !pip install transformers\n","# !pip install accelerate # requires runtime restart"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# SET LLM\n","# llm_predictor identifies chunks in the prompt to be queried from the index\n","# prompt_helper builds a prompt from the list of chunks queried from the index\n","max_input_size = 4096 # set maximum input size\n","num_outputs = 256 # set number of output tokens\n","chunk_overlap_ratio = 0.5 # max_chunk_overlap = 20  (removed)\n","chunk_size_limit = 600 # set chunk size limit\n","\n","# llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.8, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n","# prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)"]},{"cell_type":"markdown","metadata":{},"source":["**Notes**\n","* RAG Pipeline: Retrieval Augmented Generation -> Index-ing + Query-ing\n","    * The Index stage follows a step in which documents are parsed into small chunks\n","* LlamaIndex supports building a pipeline to query multiple indexes, with underlying logic connecting the indexes\n","* ServiceContext: is a supporting module that holds attributes of index- and query-ingm such as the LLM used and the number of chunks used as context in the prompt\n","* It is possible to expand from an existing index by adding new documents (chunks) or by adding metadata.\n","    * https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/usage_pattern.html#index-construction"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from llama_index import VectorStoreIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper\n","from llama_index import StorageContext, load_index_from_storage\n","from langchain import OpenAI\n","import os\n","from transformers import pipeline\n","\n","# SET PATHS\n","root_dir_local = '/Users/brauliopf/'\n","root_dir_cloud = '/Users/brauliopf/brauliopf@gmail.com - Google Drive/My Drive/'\n","root_dir_cloudUM = '/Users/brauliopf/braulio@umich.edu - Google Drive/My Drive/'\n","# os.listdir(root_dir_local) # list all files in the directory\n","\n","# LOAD DOCS\n","documents = SimpleDirectoryReader(root_dir_local + 'Documents/Dev/old/100-MLBook/Test').load_data()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# # IF TRYING THIS... remember to pass the service_context when building the index\n","\n","# # USE HUGGINGFACE LLM\n","# from llama_index.prompts.prompts import SimpleInputPrompt\n","\n","\n","# system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n","# - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n","# - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n","# - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n","# - StableLM will refuse to participate in anything that could harm a human.\n","# \"\"\"\n","\n","# # This will wrap the default prompts that are internal to llama-index\n","# query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n","\n","# import torch\n","# from llama_index.llms import HuggingFaceLLM\n","# llm = HuggingFaceLLM(\n","#     context_window=4096, \n","#     max_new_tokens=256,\n","#     generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n","#     system_prompt=system_prompt,\n","#     query_wrapper_prompt=query_wrapper_prompt,\n","#     tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n","#     model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n","#     device_map=\"auto\",\n","#     stopping_ids=[50278, 50279, 50277, 1, 0],\n","#     tokenizer_kwargs={\"max_length\": 4096},\n","#     # uncomment this if using CUDA to reduce memory usage\n","#     # model_kwargs={\"torch_dtype\": torch.float16}\n","# )\n","# service_context = ServiceContext.from_defaults(\n","#     chunk_size=1024, \n","#     llm=llm,\n","# )"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Index loaded from storage\n"]}],"source":["# LOAD/BUILD INDEX\n","try:\n","    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n","    index = load_index_from_storage(storage_context)\n","    print(\"Index loaded from storage\")\n","except:\n","    # This will create (or replace) a \"./storage\" folder and save the index there\n","    index = VectorStoreIndex.from_documents(documents,\n","                                            # service_context=service_context # use if custom LLM and not using global\n","                                            )\n","    index.storage_context.persist() # Save your index to a index.json file\n","    print(\"No index found in storage. Created a new one.\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","The types of Machine Learning algorithms are:\n","1. Supervised Learning: Used to build models that take a feature vector as input and output information that allows deducing the label for this feature vector.\n","2. Unsupervised Learning: Used to create a model that takes a feature vector as input and either transforms it into another vector or into a value that can be used to solve a practical problem.\n","3. Semi-Supervised Learning: Used to find a better model by using many unlabeled examples.\n","4. Reinforcement Learning: Used to learn a policy that takes the feature vector of a state as input and outputs an optimal action to execute in that state.\n"]}],"source":["# QUERY THE INDEX\n","\n","# generate a question answering experience\n","query_engine = index.as_query_engine(similarity_top_k=5) # use 5 most similar as context in prompt. defauts to 2\n","# generate a chat experience\n","# query_engine = index.as_chat_engine(similarity_top_k=5) # used to generate a chat experience\n","\n","response = query_engine.query(\"List the types of Machine Learning algorithms and their use cases.\")\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO3Be2+9HCXLuZCLdfsGWJV","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
